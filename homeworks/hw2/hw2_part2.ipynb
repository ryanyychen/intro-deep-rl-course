{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKTBPBJy-MIp"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "In Part 2 of HW2, we will be implementing neural Q-learning, a deep learning approach to the Q-learning algorithm.\n",
    "\n",
    "**Deep Q-Network (DQN)** is a reinforcement learning algorithm that combines Q-learning with deep neural networks, enabling it to learn optimal policies in complex high-dimensional environments. DQNs employ two neural networks:\n",
    "\n",
    "- The Q-network (main network): Trained frequently during the learning process to approximate the action-value function.\n",
    "- The target network: Used to compute target Q-values. Provides stable target Q-values for the Q-network updates. Periodically copied from the Q-network.\n",
    "\n",
    "During training, the Q-network is updated by minimizing the difference between its predicted Q-values and the target Q-values (which are computed by the target network). This separation reduces instability and divergence, which are common challenges in reinforcement learning with function approximation.\n",
    "\n",
    "The **DQN agent** interacts with the environment, collecting experiences in the form of transitions (s,a,r,s′) and storing them in the *replay buffer*. During training, mini-batches of experiences are sampled and used to train the Q-network.\n",
    "\n",
    "In Part 2, you will implement **DQN** and **DQN agent** which effectively operate on the TextWorld environment. This assignment is divided into the following steps:\n",
    "\n",
    "- Step 1: Implement `DQN`, a neural network used for approximating the Q-function. In this assignment, you will work with a RNN-based text encoder to obtain state representations.\n",
    "- Step 2: Implement `DQNAgent`, an agent containing `DQN`, interacts with environment, saves experiences to the replay buffer, and trains the Q-network.\n",
    "- Step 3: Implement the `run_policy` function. Similar to Part 1. Update the code to work with `DQNAgent`\n",
    "\n",
    "After completing steps 1 through 3, you will test your DQN agent on the same environment and testing suite as in Part 1.\n",
    "\n",
    "**Notes:**\n",
    "- We encourage you to finish Part 1 first before starting Part 2.\n",
    "- All the test configurations of the environment are the same as in Part 1.\n",
    "- Training and testing with a neural network require computations, which may require purchasing API credits. Here are some tips to minimize costs:  \n",
    "    - Implementation and initial testing can be done on CPUs. You can use your local machine or free Colab credits.\n",
    "    - For parameter tuning, you may need more computational power, especially GPUs. Instead of an exhaustive search, try making educated guesses to optimize your parameters efficiently.\n",
    "    - [lightning.ai](https://lightning.ai) also offers free GPU credits.  \n",
    "- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# EXPORT` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMJadvTO-O7z"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdVL2D-B-qYC"
   },
   "source": [
    "Install the `TextWorld-Express` engine, `graphviz` and `pydot` for visualization, and `torch` for neural network implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:45:27.272639Z",
     "iopub.status.busy": "2025-05-08T20:45:27.272417Z",
     "iopub.status.idle": "2025-05-08T20:46:52.932232Z",
     "shell.execute_reply": "2025-05-08T20:46:52.931277Z",
     "shell.execute_reply.started": "2025-05-08T20:45:27.272624Z"
    },
    "id": "Q_jkFnjg-C7N",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting textworld-express\n",
      "  Downloading textworld_express-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from textworld-express) (0.10.9.7)\n",
      "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from textworld-express) (3.10.15)\n",
      "Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.11/dist-packages (from textworld-express) (3.0.50)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit->textworld-express) (0.2.13)\n",
      "Downloading textworld_express-1.0.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textworld-express\n",
      "Successfully installed textworld-express-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.11/dist-packages (from pydot) (3.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium\n",
    "%pip install textworld-express\n",
    "%pip install graphviz\n",
    "%pip install pydot\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0GaE8xE-T6A"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:52.918085Z",
     "iopub.status.busy": "2025-05-08T20:47:52.917728Z",
     "iopub.status.idle": "2025-05-08T20:47:52.922238Z",
     "shell.execute_reply": "2025-05-08T20:47:52.921618Z",
     "shell.execute_reply.started": "2025-05-08T20:47:52.918067Z"
    },
    "id": "_ScAIKmf-Vmm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# imports for environment\n",
    "from textworld_express import TextWorldExpressEnv\n",
    "import gymnasium\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# imports for DQN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U9ElZek-y5I"
   },
   "source": [
    "# Load a Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg_yOwlT_ByQ"
   },
   "source": [
    "Set the random seed for repeatablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:54.537310Z",
     "iopub.status.busy": "2025-05-08T20:47:54.536596Z",
     "iopub.status.idle": "2025-05-08T20:47:54.540304Z",
     "shell.execute_reply": "2025-05-08T20:47:54.539683Z",
     "shell.execute_reply.started": "2025-05-08T20:47:54.537286Z"
    },
    "id": "cIGNj42d-93N",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "SEED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8_CqmGx_GOo"
   },
   "source": [
    "Initialize the game environment. `ENV` is a global that encapulates the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:55.181874Z",
     "iopub.status.busy": "2025-05-08T20:47:55.181284Z",
     "iopub.status.idle": "2025-05-08T20:47:55.663992Z",
     "shell.execute_reply": "2025-05-08T20:47:55.663257Z",
     "shell.execute_reply.started": "2025-05-08T20:47:55.181855Z"
    },
    "id": "CRwKCeqK_JOy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ENV = TextWorldExpressEnv(envStepLimit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtJDssBp_Kd_"
   },
   "source": [
    "Set the game generator to generate a particular game (coin game or map reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:55.665439Z",
     "iopub.status.busy": "2025-05-08T20:47:55.665141Z",
     "iopub.status.idle": "2025-05-08T20:47:56.002374Z",
     "shell.execute_reply": "2025-05-08T20:47:56.001527Z",
     "shell.execute_reply.started": "2025-05-08T20:47:55.665415Z"
    },
    "id": "piAle474_SPG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GAME_TYPE = \"coin\"\n",
    "GAME_PARAMS = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n",
    "ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)\n",
    "obs, infos = ENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHjBXFzibZAj"
   },
   "source": [
    "# Utility Functions\n",
    "\n",
    "This section defines utility functions for Part 2. Most of these functions do not require modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SkcfE1CA_a4"
   },
   "source": [
    "Environment Interaction Functions (see the description in Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:57.902057Z",
     "iopub.status.busy": "2025-05-08T20:47:57.901774Z",
     "iopub.status.idle": "2025-05-08T20:47:57.907895Z",
     "shell.execute_reply": "2025-05-08T20:47:57.906914Z",
     "shell.execute_reply.started": "2025-05-08T20:47:57.902022Z"
    },
    "id": "e5zs1sspRvDy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def reset_mdp(env):\n",
    "  obs, infos = env.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n",
    "  valids = infos['validActions']\n",
    "  valids.remove('inventory')\n",
    "  valids.remove('look around')\n",
    "  inv = infos['inventory']\n",
    "  modified_obs = obs_with_inventory(infos['look'], inv)\n",
    "  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), valids\n",
    "  return {'observation': infos['look'],\n",
    "          'inventory': infos['inventory'],\n",
    "          'valid actions': valids}\n",
    "\n",
    "\n",
    "def do_action_mdp(action, env):\n",
    "  obs, reward, done, infos = env.step(action)\n",
    "  #obs_look, reward_look, done_look, infos_look = env.step('look around')\n",
    "  valid_actions = infos['validActions']\n",
    "  valid_actions.remove('inventory')\n",
    "  valid_actions.remove('look around')\n",
    "  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), reward, done, valid_actions\n",
    "  return infos['look'], reward, done, {'observation': infos['look'],\n",
    "                                       'inventory': infos['inventory'],\n",
    "                                       'valid actions': valid_actions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKVzzXoVldvm"
   },
   "source": [
    "Define `pad_sequences`, which is used in Step 2 to pad text sequences into the same size for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:58.233212Z",
     "iopub.status.busy": "2025-05-08T20:47:58.232335Z",
     "iopub.status.idle": "2025-05-08T20:47:58.241923Z",
     "shell.execute_reply": "2025-05-08T20:47:58.241150Z",
     "shell.execute_reply.started": "2025-05-08T20:47:58.233162Z"
    },
    "id": "p3sUHa7klfta",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32', value=0.):\n",
    "    '''\n",
    "    Partially borrowed from Keras\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        # pre truncating\n",
    "        trunc = s[-maxlen:]\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "        # post padding\n",
    "        x[idx, :len(trunc)] = trunc\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkVHzerAk4Y3"
   },
   "source": [
    "The action set used in Part 2. This mapping table is used when we initialize DQN to set the number of final outputs (action space) and to map action strings to numerical IDs and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:47:58.531646Z",
     "iopub.status.busy": "2025-05-08T20:47:58.530852Z",
     "iopub.status.idle": "2025-05-08T20:47:58.540241Z",
     "shell.execute_reply": "2025-05-08T20:47:58.539391Z",
     "shell.execute_reply.started": "2025-05-08T20:47:58.531608Z"
    },
    "id": "3LZ2EKgWkn_X",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "action_set = {\n",
    "  'look around': 0,\n",
    "  'close door to west': 1,\n",
    "  'close door to east': 2,\n",
    "  'close door to south': 3,\n",
    "  'close door to north': 4,\n",
    "  'move west': 5,\n",
    "  'move east': 6,\n",
    "  'move south': 7,\n",
    "  'move north': 8,\n",
    "  'open door to west': 9,\n",
    "  'open door to east': 10,\n",
    "  'open door to south': 11,\n",
    "  'open door to north': 12,\n",
    "  'inventory': 13,\n",
    "  'take coin': 14,\n",
    "  'read map': 15,\n",
    "  'put map in box': 16,\n",
    "  'task': 17,\n",
    "  'take map': 18,\n",
    "  'put coin in box': 19\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn5CeI-VtYiG"
   },
   "source": [
    "In Part 2, we need to encode textual states, which is more computationally expensive than using a simple Q-table. To improve efficiency, we use some tricks to reduce the length of state representations. We recommend using this `obs_with_inventory` function for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:48:01.416093Z",
     "iopub.status.busy": "2025-05-08T20:48:01.415780Z",
     "iopub.status.idle": "2025-05-08T20:48:01.420091Z",
     "shell.execute_reply": "2025-05-08T20:48:01.419476Z",
     "shell.execute_reply.started": "2025-05-08T20:48:01.416075Z"
    },
    "id": "D1ynrEpntZEZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def obs_with_inventory(obs, inv):\n",
    "  # some tricks to reduce the length of state\n",
    "  if 'Your inventory is currently empty' in inv:\n",
    "    inv = 'Inventory: empty'\n",
    "\n",
    "  if '(maximum capacity is 2 items)' in inv:\n",
    "    inv = inv.replace(\"(maximum capacity is 2 items)\", \"\")\n",
    "\n",
    "  return obs + '\\n' + inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJzznPc_wswe"
   },
   "source": [
    "# Important Notes for this Assignment\n",
    "\n",
    "\n",
    "*   A successful episode from the MDP will give a reward of 1.0\n",
    "*   A partially successful episode from an MDP environment will give a reward of 0.5\n",
    "*   If you increase NUM_EPISODES too high, it will take too long in the autograder.\n",
    "*   We will be checking for hard coded values / outputs, so please don't take any shortcuts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sGpjl4N4YP9"
   },
   "source": [
    "# Step 1. Implement `DQN`\n",
    "\n",
    "In this step, we will define how the neural network will encode observations and calculate Q-values to approximate the Q-function.\n",
    "We implement the `DQN` class that estimates the expected Q-values for each possible action in a given state.\n",
    "Since neural networks in the DQN takes inputs in the form of tensor, we need to encode a state to obtain state representations. We adopt a simple RNN-based state network following the paper [Interactive Fiction Games: A Colossal Adventure](https://arxiv.org/pdf/1909.05398) to encode textual states from the Textworld-Express.\n",
    "\n",
    "To help you with this task, we have provided the following three classes:\n",
    "- `PackedEncoderRNN`: This class is a recurrent neural network (RNN) for processing sequential data like text. You don't need to modify this class in this assignment.\n",
    "- `StateNetwork`: This class encodes the observations and inventory information from the TextWorld game, creating a compact representation of the game state. While the current implementation uses one RNN to encode the state, you can optionally explore using more RNNs (as suggested in the [paper](https://arxiv.org/pdf/1909.05398)) to encode observations and inventory separately, concatenating them for the final state representations.\n",
    "- `DQN`: This is the core of the deep Q-Network, containing the `StateNetwork`. **Your main task in this step is to complete the `DQN` class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T20:48:01.960917Z",
     "iopub.status.busy": "2025-05-08T20:48:01.960664Z",
     "iopub.status.idle": "2025-05-08T20:48:01.972511Z",
     "shell.execute_reply": "2025-05-08T20:48:01.971654Z",
     "shell.execute_reply.started": "2025-05-08T20:48:01.960900Z"
    },
    "id": "3ejeYWxRrtiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class PackedEncoderRNN(nn.Module):\n",
    "  \"\"\"\n",
    "    No need to change, but feel free to improve if needed.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(PackedEncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "  def forward(self, input, hidden=None):\n",
    "    embedded = self.embedding(input).permute(1, 0, 2) # T x Batch x EmbDim\n",
    "    if hidden is None:\n",
    "        hidden = self.initHidden(input.size(0))\n",
    "\n",
    "    # Pack the padded batch of sequences\n",
    "    lengths = torch.tensor([torch.nonzero(n)[-1] + 1 for n in input], dtype=torch.long).cpu()\n",
    "    packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, enforce_sorted=False)\n",
    "    output, hidden = self.gru(packed, hidden)\n",
    "\n",
    "    # Unpack the padded sequence\n",
    "    output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "\n",
    "    # Return only the last timestep of output for each sequence\n",
    "    lengths = lengths.cuda()\n",
    "    idx = (lengths-1).view(-1, 1).expand(len(lengths), output.size(2)).unsqueeze(0)\n",
    "    output = output.gather(0, idx).squeeze(0)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self, batch_size):\n",
    "    return torch.zeros(1, batch_size, self.hidden_size).cuda()\n",
    "\n",
    "\n",
    "class StateNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "    No need to change, but feel free to improve if needed.\n",
    "  \"\"\"\n",
    "  def __init__(self, config):\n",
    "    super(StateNetwork, self).__init__()\n",
    "    self.config = config\n",
    "    self.enc_state = PackedEncoderRNN(config.vocab_size, config.hidden_size)\n",
    "    self.fcx = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.fch = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batch_size = inputs.shape[0]\n",
    "    x_o, h_o = self.enc_state(inputs, self.enc_state.initHidden(batch_size))\n",
    "\n",
    "    x = F.relu(self.fcx(x_o))\n",
    "    h = F.relu(self.fch(h_o))\n",
    "\n",
    "    return x, h\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(DQN, self).__init__()\n",
    "    self.state_network = StateNetwork(config)\n",
    "    self.act_scorer = nn.Linear(config.hidden_size, config.act_size)\n",
    "\n",
    "  def forward(self, state):\n",
    "    \"\"\"\n",
    "      the output should be (BATCH_SIZE, ACTION_SIZE): the estimated Q-values for each action given a state\n",
    "    \"\"\"\n",
    "    ### YOUR CODE BELOW HERE\n",
    "    # Get the state representation\n",
    "    x, _ = self.state_network(state)\n",
    "\n",
    "    # Compute the Q-values for each action\n",
    "    q_values = self.act_scorer(x)\n",
    "\n",
    "    # Return the Q-values\n",
    "    return q_values\n",
    "    ### YOUR CODE ABOVE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt3zgQxTX0x0"
   },
   "source": [
    "Test your DQN implementation with a simple input example. This is a sanity check and does not guarantee the correctness of your code. You will test your implementation after Step 2 and Step 3 on the actual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T20:48:02.658735Z",
     "iopub.status.busy": "2025-05-08T20:48:02.658499Z",
     "iopub.status.idle": "2025-05-08T20:48:03.447002Z",
     "shell.execute_reply": "2025-05-08T20:48:03.446251Z",
     "shell.execute_reply.started": "2025-05-08T20:48:02.658718Z"
    },
    "id": "FoRXWfrsXx3p",
    "outputId": "d161fe26-4d23-4340-bb85-da4f04be1438",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "******** Q-values ******** (not trained)\n",
      "look around         : -0.069249\n",
      "close door to west  : 0.006108\n",
      "close door to east  : 0.013082\n",
      "close door to south : 0.065874\n",
      "close door to north : -0.104299\n",
      "move west           : 0.018191\n",
      "move east           : 0.048440\n",
      "move south          : -0.165974\n",
      "move north          : -0.158540\n",
      "open door to west   : 0.107757\n",
      "open door to east   : 0.096234\n",
      "open door to south  : 0.025489\n",
      "open door to north  : -0.106253\n",
      "inventory           : -0.035402\n",
      "take coin           : -0.043764\n",
      "read map            : 0.204055\n",
      "put map in box      : -0.065001\n",
      "task                : -0.085677\n",
      "take map            : 0.042167\n",
      "put coin in box     : -0.030128\n"
     ]
    }
   ],
   "source": [
    "# Test your DQN implementation\n",
    "\n",
    "class DQNConfig:\n",
    "  vocab_size = 50257 # vocab size of the GPT2 tokenizer. Change only if you want to try a different tokenizer.\n",
    "  act_size = len(action_set)\n",
    "  embedding_size = 64\n",
    "  hidden_size = 256\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "config = DQNConfig()\n",
    "dqn = DQN(config).to(device)\n",
    "x = torch.tensor([0, 1, 2, 3, 4, 5]).to(device).unsqueeze(0) # random input token ids\n",
    "print(\"******** Q-values ******** (not trained)\")\n",
    "q_values = dqn(x)\n",
    "for act, actid in action_set.items():\n",
    "  print(f\"{act:20}: {q_values[0][actid]:0.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhgPPbfb9ANG"
   },
   "source": [
    "# Step 2: Implement `DQNAgent`\n",
    "\n",
    "In this step, you will implement the `DQNAgent` class, which encapsulates the `DQN` model and the core logic for interacting with the environment, storing experiences, and training the neural network. The `DQNAgent` is responsible for:\n",
    "\n",
    "* Interacting with the environment: Using `reset_mdp()` to initialize an episode and `do_action_mdp()` to take actions and observe the consequences.\n",
    "* Storing experiences: Saving transitions (state, action, reward, next state, done) in a replay buffer for experience replay.\n",
    "* Estimating Q-values: Using the `DQN` model to predict the expected Q-values for each action in a given state.\n",
    "* Training the neural network: Updating the `DQN` model's parameters based on the experiences stored in the replay buffer.\n",
    "\n",
    "Note that the `train` function will serve as the entry point for the training process. For example, we will use this code to create and train your agent:\n",
    "\n",
    "```\n",
    "agent = DQNAgent(action_set, DQNConfig(), gamma=GAMMA, epsilon=EPSILON)\n",
    "agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
    "```\n",
    "\n",
    "The `train` function takes the following arguments:\n",
    "\n",
    "* `env`: The TextWorldExpress environment (`ENV`).\n",
    "* `num_episodes`: The total number of episodes to train for.\n",
    "* `threshold`: The maximum number of steps allowed in a single episode.\n",
    "\n",
    "**Design Considerations:**\n",
    "\n",
    "Apart from the `train` function, you have flexibility in designing the internal structure and methods of the `DQNAgent` class. Here are some recommendations to guide your implementation:\n",
    "\n",
    "* Epsilon Decay: Gradually decrease the exploration rate (epsilon) over time to shift from exploration to exploitation. (use `epsilon_decay` and `epsilon_min`)\n",
    "* Target Network Update: Periodically update the target network with the weights of the main Q-network (e.g., every 1000 steps) to stabilize training. (use `update_freq_target`)\n",
    "* Q-Network Update Frequency: Update the Q-network every few steps (e.g., every 4 steps) rather than after every single step to improve efficiency and stability. (use `update_freq` )\n",
    "\n",
    "Note that your function will interact with the environment through `reset_mdp()` and `do_action_mdp()`. Be sure to reset the environment before running, and terminate the episode if `do_action_mdp()` indicates the episode has terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:09:57.928967Z",
     "iopub.status.busy": "2025-05-08T21:09:57.928279Z",
     "iopub.status.idle": "2025-05-08T21:09:57.942569Z",
     "shell.execute_reply": "2025-05-08T21:09:57.941974Z",
     "shell.execute_reply.started": "2025-05-08T21:09:57.928946Z"
    },
    "id": "R02kCLNnrvUa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class DQNAgent:\n",
    "  def __init__(self,\n",
    "               action_set,\n",
    "               dqn_config,\n",
    "               gamma,\n",
    "               epsilon,\n",
    "               learning_rate=0.0005,\n",
    "               epsilon_decay=0.995,\n",
    "               epsilon_min=0.01,\n",
    "               batch_size=64,\n",
    "               memory_size=100000,\n",
    "               update_freq=4,\n",
    "               update_freq_target=1000):\n",
    "    self.act2id = {a: i for i, a in enumerate(action_set)}\n",
    "    self.id2act = {i: a for i, a in enumerate(action_set)}\n",
    "\n",
    "    self.update_freq = update_freq\n",
    "    self.update_freq_target = update_freq_target\n",
    "    self.max_seq_len = 256  # DO NOT CHANGE `max_seq_len`\n",
    "    self.tokenizer =  AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    self.gamma = gamma\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.batch_size = batch_size\n",
    "    self.replay_buffer = deque(maxlen=memory_size)\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.model = DQN(dqn_config).to(self.device)\n",
    "    self.target_model = DQN(dqn_config).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "  def tokenize_and_pad_states(self, states):\n",
    "    \"\"\"\n",
    "    Tokenizes and pads a list of textual states.\n",
    "    Uses the pre-trained tokenizer to convert textual descriptions of\n",
    "    the environment into numerical representations (tokens) and then pads\n",
    "    the sequences to a uniform length.\n",
    "\n",
    "    Args:\n",
    "        states: A list of string representations of the environment state.\n",
    "\n",
    "    Returns:\n",
    "        A padded NumPy array of tokenized states.\n",
    "    \"\"\"\n",
    "    input_ids = self.tokenizer(states)['input_ids']\n",
    "    return pad_sequences(input_ids, maxlen=self.max_seq_len)\n",
    "\n",
    "  def train(self, env, num_episodes, threshold):\n",
    "    \"\"\"Trains the DQN agent in the given environment.\n",
    "\n",
    "    Args:\n",
    "      env: The environment to train the agent in.\n",
    "      num_episodes: The number of episodes to train for.\n",
    "      threshold: The maximum number of steps to take in each episode.\n",
    "\n",
    "    Returns:\n",
    "      - A list of rewards obtained in each episode.\n",
    "    \"\"\"\n",
    "    all_rewards = []  # Store rewards for each episode\n",
    "    ### YOUR CODE BELOW HERE\n",
    "    global_steps = 0\n",
    "    for ep in range(num_episodes):\n",
    "      state = reset_mdp(env)\n",
    "      total_reward = 0\n",
    "      i = 0\n",
    "      done = False\n",
    "      while not done and i < threshold:\n",
    "        obs_inv = obs_with_inventory(state['observation'], state['inventory'])\n",
    "        # e-greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "          action = random.choice(state['valid actions'])\n",
    "        else:\n",
    "          q_values = self.model(torch.tensor(self.tokenize_and_pad_states([obs_inv])).to(self.device))\n",
    "          action = self.id2act[torch.argmax(q_values).item()]\n",
    "\n",
    "        _, reward, done, next_state = do_action_mdp(action, env)\n",
    "        total_reward += reward\n",
    "        next_obs_inv = obs_with_inventory(next_state['observation'], next_state['inventory'])\n",
    "        self.replay_buffer.append((obs_inv, action, reward, next_obs_inv, done))\n",
    "\n",
    "        if len(self.replay_buffer) >= self.batch_size and i % self.update_freq == 0:\n",
    "          batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "          states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "          # Convert all to tensors\n",
    "          states = torch.tensor(self.tokenize_and_pad_states(states)).to(self.device)\n",
    "          actions = torch.tensor([self.act2id[action] for action in actions]).to(self.device)\n",
    "          rewards = torch.tensor(rewards).to(self.device)\n",
    "          next_states = torch.tensor(self.tokenize_and_pad_states(next_states)).to(self.device)\n",
    "          dones = torch.tensor([1 - int(done) for done in dones]).to(self.device)\n",
    "\n",
    "          q_values_all = self.model(states)\n",
    "          q_values = q_values_all.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "          next_q_values = self.target_model(next_states).max(axis=1)\n",
    "\n",
    "          targets = rewards + self.gamma * next_q_values.values * dones\n",
    "          loss = F.mse_loss(q_values, targets)\n",
    "\n",
    "          self.optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "        if global_steps % self.update_freq_target == 0:\n",
    "          self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        state = next_state\n",
    "        i += 1\n",
    "        global_steps += 1\n",
    "\n",
    "      all_rewards.append(total_reward)\n",
    "      self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    ### YOUR CODE ABOVE HERE\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js7x9FBAr_0e"
   },
   "source": [
    "Similar to Part 1, you might need to adjust the hyperparameters NUM_EPISODES, THRESHOLD, GAMMA, and EPSILON from their default values. These variables are just for the simple test below. The autograder will use the variables you set in `set_parameters` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:09:58.564057Z",
     "iopub.status.busy": "2025-05-08T21:09:58.563824Z",
     "iopub.status.idle": "2025-05-08T21:09:58.567701Z",
     "shell.execute_reply": "2025-05-08T21:09:58.567058Z",
     "shell.execute_reply.started": "2025-05-08T21:09:58.564041Z"
    },
    "id": "fvCsvElH0TSD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "NUM_EPISODES = 100\n",
    "THRESHOLD = 200\n",
    "GAMMA = 0.8\n",
    "EPSILON = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLXFMIjwbZAk"
   },
   "source": [
    "Create a DQNAgent and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T20:48:06.747392Z",
     "iopub.status.busy": "2025-05-08T20:48:06.746998Z",
     "iopub.status.idle": "2025-05-08T20:49:03.901428Z",
     "shell.execute_reply": "2025-05-08T20:49:03.900653Z",
     "shell.execute_reply.started": "2025-05-08T20:48:06.747367Z"
    },
    "id": "6YVTbsgzbZAk",
    "outputId": "4d0dc439-bc50-4ad5-d982-9e1516a1c0b7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65e4d2832924b808055442bed6cb638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad45abdf1224a4a85d181633c142391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cddda75da248d1b7dcba76a844b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e007c0ecd6304ddf9c22e87f44c1fb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1662c9cb9b34493b329ad6c05c1d4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(action_set, DQNConfig(), gamma=GAMMA, epsilon=EPSILON)\n",
    "all_rewards = agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
    "print(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z2k4EkrGaMm"
   },
   "source": [
    "# Step 3. Implement Code to Run a Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j5OrjrTtxWM"
   },
   "source": [
    "In this step, you will implement the `run_policy` function to execute the policy learned by the `DQNAgent`. If you have successfully completed Part 1, adapting your existing implementation to work with the `DQNAgent` should be straightforward.\n",
    "\n",
    "**Important:** Ensure that your agent uses **greedy action selection** during policy execution, meaning it always chooses the action with the highest estimated Q-value from the `DQN`. You may implement helper functions within the `DQNAgent` class to facilitate policy execution.\n",
    "\n",
    "The `run_policy` function takes the following arguments:\n",
    "\n",
    "* `agent`: Your trained `DQNAgent` instance.\n",
    "* `env`: The TextWorldExpress environment (e.g., `ENV`).\n",
    "* `threshold`: The maximum number of steps allowed in an episode before termination.\n",
    "\n",
    "Your function should run a single episode from the initial state and return:\n",
    "- A list of actions taken during the episode (e.g., `[act_1, act_2, ... act_n]`).\n",
    "- The total sum reward of all actions taken as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:01.675232Z",
     "iopub.status.busy": "2025-05-08T21:10:01.674644Z",
     "iopub.status.idle": "2025-05-08T21:10:01.680274Z",
     "shell.execute_reply": "2025-05-08T21:10:01.679480Z",
     "shell.execute_reply.started": "2025-05-08T21:10:01.675185Z"
    },
    "id": "7jJCxMBTGdv9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def run_policy(agent, env, threshold=50):\n",
    "  actions = [] # Store the entire sequence of actions here\n",
    "  total_reward = 0.0 # Store the total sum reward of all actions executed here\n",
    "  ### YOUR CODE BELOW HERE\n",
    "  state = reset_mdp(env)\n",
    "  i = 0\n",
    "  done = False\n",
    "  while not done and i < threshold:\n",
    "    obs_inv = obs_with_inventory(state['observation'], state['inventory'])\n",
    "    q_values = agent.model(torch.tensor(agent.tokenize_and_pad_states([obs_inv])).to(agent.device))\n",
    "    action = agent.id2act[torch.argmax(q_values).item()]\n",
    "    actions.append(action)\n",
    "    _, reward, done, next_state = do_action_mdp(action, env)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    i += 1\n",
    "  ### YOUR CODE ABOVE HERE\n",
    "  return actions, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goBOBYvBYzeR"
   },
   "source": [
    "Test your `run_policy` function. Set the threshold value for episode length during policy execution (test time threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:02.250496Z",
     "iopub.status.busy": "2025-05-08T21:10:02.249898Z",
     "iopub.status.idle": "2025-05-08T21:10:02.253479Z",
     "shell.execute_reply": "2025-05-08T21:10:02.252905Z",
     "shell.execute_reply.started": "2025-05-08T21:10:02.250478Z"
    },
    "id": "KpK_2wuMuZ7d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "TEST_THRESHOLD = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWOGP5aWY59n"
   },
   "source": [
    "Run the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:02.533753Z",
     "iopub.status.busy": "2025-05-08T21:10:02.533586Z",
     "iopub.status.idle": "2025-05-08T21:10:02.561548Z",
     "shell.execute_reply": "2025-05-08T21:10:02.560651Z",
     "shell.execute_reply.started": "2025-05-08T21:10:02.533741Z"
    },
    "id": "vhrGyDGBGrMP",
    "outputId": "b5d394af-09d1-4779-9f91-0a42ebcdc4d6",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o2.generateNewGameJSON",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2781028283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plan:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total reward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/818555820.py\u001b[0m in \u001b[0;36mrun_policy\u001b[0;34m(agent, env, threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;31m# Store the total sum reward of all actions executed here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m### YOUR CODE BELOW HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreset_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1157921979.py\u001b[0m in \u001b[0;36mreset_mdp\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgameFold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerateGoldPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mvalids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validActions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inventory'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/textworld_express/textworld_express.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, gameFold, gameName, gameParams, generateGoldPath)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresetWithRandomSeedJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerateGoldPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateNewGameJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerateGoldPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseJSONResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o2.generateNewGameJSON"
     ]
    }
   ],
   "source": [
    "plan, total_reward = run_policy(agent, ENV, threshold = TEST_THRESHOLD)\n",
    "print(\"plan:\", plan)\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cBfmUDG1XHk"
   },
   "source": [
    "# New Environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLeYTiUVY9Tj"
   },
   "source": [
    "The following cells are the same as in Part 1: creating new environemnts: `StochasticTextWorldExpressEnv` and `PunishmentTextWorldExpressEnv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:03.003346Z",
     "iopub.status.busy": "2025-05-08T21:10:03.002977Z",
     "iopub.status.idle": "2025-05-08T21:10:03.006513Z",
     "shell.execute_reply": "2025-05-08T21:10:03.005898Z",
     "shell.execute_reply.started": "2025-05-08T21:10:03.003331Z"
    },
    "id": "n-hc6VnL793a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NEVER_PICK_ACTIONS = set(['look around', 'inventory'])\n",
    "ENV_VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:03.150958Z",
     "iopub.status.busy": "2025-05-08T21:10:03.150756Z",
     "iopub.status.idle": "2025-05-08T21:10:03.158977Z",
     "shell.execute_reply": "2025-05-08T21:10:03.158248Z",
     "shell.execute_reply.started": "2025-05-08T21:10:03.150944Z"
    },
    "id": "GsQWZj0H1WeS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StochasticTextWorldExpressEnv(TextWorldExpressEnv):\n",
    "\n",
    "  def __init__(self, serverPath=None, envStepLimit=100, stochasticity = 0.0):\n",
    "    # Call the super constructor\n",
    "    super().__init__(serverPath, envStepLimit)\n",
    "    # Store the valid actions and stochasticity\n",
    "    self.valid_actions = []\n",
    "    self.stochasticity = stochasticity\n",
    "\n",
    "  def reset(self, seed=None, gameFold=None, gameName=None, gameParams=None, generateGoldPath=False):\n",
    "    # Call the super method\n",
    "    observation, infos = super().reset(seed, gameFold, gameName, gameParams, generateGoldPath)\n",
    "    # Update the valid actions\n",
    "    self.valid_actions = infos['validActions']\n",
    "    return observation, infos\n",
    "\n",
    "  def step(self, action:str):\n",
    "    # If a random value is less than the stochasticity target, choose a random action\n",
    "    if random.random() < self.stochasticity:\n",
    "      temp_valids = copy.deepcopy(self.valid_actions)\n",
    "      # Remove inventory and look around from valid actions to choose from\n",
    "      temp_valids = list(set(self.valid_actions).difference(NEVER_PICK_ACTIONS))\n",
    "      # Pick a random action from whatever remains\n",
    "      action = random.choice(temp_valids)\n",
    "    # If debugging flag is on, print the action that will be executed\n",
    "    if ENV_VERBOSE:\n",
    "      print(\"[[action]]:\", action)\n",
    "    # Call the super class with either the action passed in or the randomly chosen one\n",
    "    observation, reward, isCompleted, infos = super().step(action)\n",
    "    # Update the valid actions\n",
    "    self.valid_actions = infos['validActions']\n",
    "    return observation, reward, isCompleted, infos\n",
    "\n",
    "class PunishmentTextWorldExpressEnv(TextWorldExpressEnv):\n",
    "\n",
    "  def __init__(self, serverPath=None, envStepLimit=100, punishment = 0.0):\n",
    "    # Call the super constructor\n",
    "    super().__init__(serverPath, envStepLimit)\n",
    "    # Store the punishment\n",
    "    self.punishment = punishment\n",
    "    # Store the previous observation\n",
    "    self.previous_observation = None\n",
    "\n",
    "  def step(self, action:str):\n",
    "    # Call the super method\n",
    "    observation, reward, isCompleted, infos = super().step(action)\n",
    "    # If the current look is the same as the previous look, then we have performed an illegal action\n",
    "    if infos['look'] == self.previous_observation:\n",
    "      reward = self.punishment\n",
    "    # Store the previous observation\n",
    "    self.previous_observation = infos['look']\n",
    "    return observation, reward, isCompleted, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKV4lUC1agso"
   },
   "source": [
    "New environments must be registered through the Gymnasium API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:03.496968Z",
     "iopub.status.busy": "2025-05-08T21:10:03.496756Z",
     "iopub.status.idle": "2025-05-08T21:10:03.501015Z",
     "shell.execute_reply": "2025-05-08T21:10:03.500252Z",
     "shell.execute_reply.started": "2025-05-08T21:10:03.496952Z"
    },
    "id": "OFWWPDS35bMn",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment TextWorldExpress-StochasticTextWorldExpressEnv-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment TextWorldExpress-PunishmentTextWorldExpressEnv-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# register new environment\n",
    "gymnasium.register(id='TextWorldExpress-StochasticTextWorldExpressEnv-v0',\n",
    "                   entry_point='__main__:StochasticTextWorldExpressEnv')\n",
    "gymnasium.register(id='TextWorldExpress-PunishmentTextWorldExpressEnv-v0',\n",
    "                   entry_point='__main__:PunishmentTextWorldExpressEnv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywMkXNMwQsgZ"
   },
   "source": [
    "# Testing Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR9ZHmgw8UFa"
   },
   "source": [
    "This function will run all environments, all game types, all game parameters, and all seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:03.970011Z",
     "iopub.status.busy": "2025-05-08T21:10:03.969650Z",
     "iopub.status.idle": "2025-05-08T21:10:03.977997Z",
     "shell.execute_reply": "2025-05-08T21:10:03.977256Z",
     "shell.execute_reply.started": "2025-05-08T21:10:03.969979Z"
    },
    "id": "OLy0jJ9gRA0L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_all(environments, games, seeds):\n",
    "  global ENV, GAME_TYPE, GAME_PARAMS, SEED, EPSILON_DECAY\n",
    "  # Results will contain a key (env type, game type, game params, seed) and values will be plans and total_rewards\n",
    "  results = {}\n",
    "  test_id = 0\n",
    "  total_reward = 0\n",
    "  # Iterate through all environments given\n",
    "  for env in environments:\n",
    "    # set global environment\n",
    "    ENV = env\n",
    "    # Iterate through all game types, the keys of the games dict\n",
    "    for game_type in games:\n",
    "      # Set the global game type\n",
    "      GAME_TYPE = game_type\n",
    "      # Iterate through all game parameters for the given game type in game dict\n",
    "      for params in games[game_type]:\n",
    "        # set the global game params\n",
    "        GAME_PARAMS = params\n",
    "        # load the environment\n",
    "        ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)\n",
    "        # Iterate through all seeds\n",
    "        for seed in seeds:\n",
    "          print(f\"TESTING {type(ENV)}, {GAME_TYPE}, {GAME_PARAMS}, {seed}\")\n",
    "          # set the global seed\n",
    "          SEED = seed\n",
    "\n",
    "          # Run the DQNAgent and get the policy\n",
    "          agent = DQNAgent(action_set,\n",
    "                           DQNConfig(),\n",
    "                           gamma=GAMMA,\n",
    "                           epsilon=EPSILON,\n",
    "                           epsilon_decay=EPSILON_DECAY)\n",
    "\n",
    "          agent.train(ENV, NUM_EPISODES, THRESHOLD)\n",
    "\n",
    "          # run the policy to get the plan\n",
    "          plan, reward = run_policy(agent, ENV, threshold = TEST_THRESHOLD)\n",
    "\n",
    "          test_id += 1\n",
    "          total_reward += reward\n",
    "\n",
    "          print(f\"TESTING {test_id}: total_reward {total_reward}/{test_id} \\t (reward: {reward})\")\n",
    "          # Store the plan in the results\n",
    "          results[(type(ENV), GAME_TYPE, GAME_PARAMS, SEED)] = (plan, total_reward)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:04.138140Z",
     "iopub.status.busy": "2025-05-08T21:10:04.137567Z",
     "iopub.status.idle": "2025-05-08T21:10:04.773276Z",
     "shell.execute_reply": "2025-05-08T21:10:04.772434Z",
     "shell.execute_reply.started": "2025-05-08T21:10:04.138114Z"
    },
    "id": "jeBUqaKWbBQj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seeds = list(range(3))\n",
    "environments = [TextWorldExpressEnv(envStepLimit=100),\n",
    "                StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25),\n",
    "                PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0)]\n",
    "games = {'coin':      ['numLocations=5,includeDoors=1,numDistractorItems=0',\n",
    "                       'numLocations=6,includeDoors=1,numDistractorItems=0',\n",
    "                       'numLocations=7,includeDoors=1,numDistractorItems=0',\n",
    "                       'numLocations=10,includeDoors=1,numDistractorItems=0'],\n",
    "         'mapreader': ['numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0',\n",
    "                       'numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0',\n",
    "                       'numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaZoBv5v8Qa9"
   },
   "source": [
    "Set parameters. Do not alter this cell outside of the changing the numeric values.\n",
    "\n",
    "**You might need to change these parameters to get a good result on the harder environments**\n",
    "\n",
    "Please note that increasing `NUM_EPISODES` will result in an increase in time to run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:22.012366Z",
     "iopub.status.busy": "2025-05-08T21:10:22.011665Z",
     "iopub.status.idle": "2025-05-08T21:10:22.016228Z",
     "shell.execute_reply": "2025-05-08T21:10:22.015577Z",
     "shell.execute_reply.started": "2025-05-08T21:10:22.012342Z"
    },
    "id": "-DaxveUb8PQv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def set_parameters():\n",
    "    global NUM_EPISODES, THRESHOLD, GAMMA, EPSILON, TEST_THRESHOLD, EPSILON_DECAY\n",
    "    NUM_EPISODES = 125\n",
    "    THRESHOLD = 250\n",
    "    GAMMA = 0.9\n",
    "    EPSILON = 1.0\n",
    "    TEST_THRESHOLD = 200\n",
    "    EPSILON_DECAY = 0.95\n",
    "\n",
    "    return {\n",
    "      'NUM_EPISODES': NUM_EPISODES,\n",
    "      'THRESHOLD': THRESHOLD,\n",
    "      'GAMMA': GAMMA,\n",
    "      'EPSILON': EPSILON,\n",
    "      'TEST_THRESHOLD': TEST_THRESHOLD,\n",
    "      'EPSILON_DECAY': EPSILON_DECAY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:22.330568Z",
     "iopub.status.busy": "2025-05-08T21:10:22.330348Z",
     "iopub.status.idle": "2025-05-08T21:10:22.335255Z",
     "shell.execute_reply": "2025-05-08T21:10:22.334531Z",
     "shell.execute_reply.started": "2025-05-08T21:10:22.330551Z"
    },
    "id": "T9dAzWLerjKI",
    "outputId": "055dabd0-8624-4ae0-a8e5-226a316518ae",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_EPISODES': 125,\n",
       " 'THRESHOLD': 250,\n",
       " 'GAMMA': 0.9,\n",
       " 'EPSILON': 1.0,\n",
       " 'TEST_THRESHOLD': 200,\n",
       " 'EPSILON_DECAY': 0.95}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzB7i03p8dij"
   },
   "source": [
    "Run all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T21:10:24.597645Z",
     "iopub.status.busy": "2025-05-08T21:10:24.596987Z",
     "iopub.status.idle": "2025-05-08T22:03:06.347568Z",
     "shell.execute_reply": "2025-05-08T22:03:06.346806Z",
     "shell.execute_reply.started": "2025-05-08T21:10:24.597621Z"
    },
    "id": "Xxm2giWM8fcJ",
    "outputId": "e5b2f405-09ae-4b2c-a4a2-c98ed2733fce",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 1: total_reward 1.0/1 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 2: total_reward 2.0/2 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 3: total_reward 3.0/3 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 4: total_reward 4.0/4 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 5: total_reward 5.0/5 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 6: total_reward 6.0/6 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 7: total_reward 6.0/7 \t (reward: 0.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 8: total_reward 6.0/8 \t (reward: 0.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 9: total_reward 7.0/9 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 10: total_reward 8.0/10 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 11: total_reward 9.0/11 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 12: total_reward 10.0/12 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 13: total_reward 11.0/13 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 14: total_reward 12.0/14 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 15: total_reward 12.0/15 \t (reward: 0.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 16: total_reward 13.0/16 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 17: total_reward 13.0/17 \t (reward: 0.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 18: total_reward 14.0/18 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 19: total_reward 15.0/19 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 20: total_reward 16.0/20 \t (reward: 1.0)\n",
      "TESTING <class 'textworld_express.textworld_express.TextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 21: total_reward 17.0/21 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 22: total_reward 18.0/22 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 23: total_reward 19.0/23 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 24: total_reward 20.0/24 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 25: total_reward 21.0/25 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 26: total_reward 22.0/26 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 27: total_reward 23.0/27 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 28: total_reward 24.0/28 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 29: total_reward 25.0/29 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 30: total_reward 26.0/30 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 31: total_reward 27.0/31 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 32: total_reward 28.0/32 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 33: total_reward 29.0/33 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 34: total_reward 30.0/34 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 35: total_reward 31.0/35 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 36: total_reward 32.0/36 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 37: total_reward 33.0/37 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 38: total_reward 34.0/38 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 39: total_reward 35.0/39 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 40: total_reward 36.0/40 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 41: total_reward 37.0/41 \t (reward: 1.0)\n",
      "TESTING <class '__main__.StochasticTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 42: total_reward 38.0/42 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 43: total_reward 39.0/43 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 44: total_reward 40.0/44 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=5,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 45: total_reward 41.0/45 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 46: total_reward 42.0/46 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 47: total_reward 43.0/47 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=6,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 48: total_reward 44.0/48 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 49: total_reward 45.0/49 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 50: total_reward 46.0/50 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=7,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 51: total_reward 47.0/51 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 0\n",
      "TESTING 52: total_reward 48.0/52 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 1\n",
      "TESTING 53: total_reward 49.0/53 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, coin, numLocations=10,includeDoors=1,numDistractorItems=0, 2\n",
      "TESTING 54: total_reward 50.0/54 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 55: total_reward 51.0/55 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 56: total_reward 52.0/56 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 57: total_reward 53.0/57 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 58: total_reward 54.0/58 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 59: total_reward 55.0/59 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 60: total_reward 56.0/60 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 0\n",
      "TESTING 61: total_reward 57.0/61 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 1\n",
      "TESTING 62: total_reward 58.0/62 \t (reward: 1.0)\n",
      "TESTING <class '__main__.PunishmentTextWorldExpressEnv'>, mapreader, numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0, 2\n",
      "TESTING 63: total_reward 59.0/63 \t (reward: 1.0)\n"
     ]
    }
   ],
   "source": [
    "results = run_all(environments, games, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acQaep88V13E"
   },
   "source": [
    "# Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDcg7IENXtj1"
   },
   "source": [
    "Grading will be done in the same way as Part 1. There will be a total of 63 tests: 1 point for each correct plan per algorithm.\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "Maximum total points: 50\n",
    "\n",
    "| # correct plan | Score |\n",
    "|----------|-------|\n",
    "| >= 50   |  50   |\n",
    "| 49      |  49   |\n",
    "| 48      |  48   |\n",
    "| ...      |  ...  |\n",
    "| 2      |  2   |\n",
    "| 1       |   1  |\n",
    "| 0   |   0   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA0XSuitsCP5"
   },
   "source": [
    "# Submission\n",
    "\n",
    "Upload this notebook with the name `hw2_part2.ipynb` file to Gradescope. Part 2 will be graded on our local GPU machines. Final grades will be uploaded on Gradescope after the submission deadline.\n",
    "\n",
    "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
    "\n",
    "You should ***not*** add any cells to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but you must remove them when submitting.\n",
    "\n",
    "If you identify an issue with the autograder, please feel free to reach out to us on Piazza, or email bok004@ucsd.edu."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cse190venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
